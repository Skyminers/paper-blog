---
author: Sky_miner
pubDatetime: 2024-08-13T17:40:22.000+08:00
# modDatetime: 2024-08-13T17:40:22.000+08:00
title: ResNet 面试突击
featured: false
draft: false
tags:
  - 面试
description: 简单描述了 ResNet 的结构，以及一些常见的问题。
---

## Table of contents

## ResNet 简介

> [《Deep Residual Learning for Image Recognition》](https://arxiv.org/pdf/1512.03385)

Deep Residual Network, 深度残差神经网络，简称 ResNet。通过残差学习使训练更深的网络成为可能。ResNet 有很多种不同深度的网络，ResNet18, ResNet34, ResNet50, ResNet101, ResNet152 等。

### ResNet 网络架构

结构组成：

1. 初始卷积层：初步进行特征提取
   - 初始卷积层 7x7，步长 2，padding 3，将维度减半。后面跟 BN 层和 ReLU 层。
   - 进行图像特征进行基础的提取
2. 残差块组：包含多个残差单元
   - 每个残差块可以从前一组提取的特征中提取更高级的特征
   - 通过残差连接，每个残差块能学习非线形映射
   - Skip Connection 可以更好地传递梯度
3. 全局平均池化：减少维度
   - 全局平均池化将每个特征图缩减为一个单一的值，显著减少了计算量
   - 防止过拟合，提升泛化能力。减少参数有助于防止模型过拟合
4. 全链接层：可以用于分类或其他任务
   - 可以根据任务需求根据前层特征进行分类或回归
   - 完全整合之前各层的信息，输出一个固定大小的特征向量。

### ResNet 残差单元

![](@assets/images/resnet/residual.png)

传统的 CNN 中，每个卷积层学习的都是输入与输出之间的映射。残差块则采用了不同的策略，尝试学习输入和输出之间的残差映射。

## ResNet 问答

### 为什么 ResNet 有效

通过 BN 和合理的初始化结局了梯度消失和梯度爆炸的问题。残差连接可以更容易地学到类恒等映射，这使 ResNet 的学习任务更简单，可以有效地解决梯度弥散的问题。

### ResNet152 中有什么特殊的设计

在更深的 ResNet 中，位了减少计算量通常先使用 1x1 的卷积核进行降维，然后再进行 3x3 卷积，最后再通过 1x1 卷积恢复维度。

### 请简要介绍 ResNet

ResNet 通过将多个神经网络的层聚合成一个块，然后再块的一侧加入恒等映射，使这个块从原本的 $F(x)$ 变成了 $F(x)+x$，从而解决了神经网络的退化问题。

> ResNet 引入跳跃链接，使得梯度能够更好地回传，从而缓解了梯度消失与梯度爆炸问题。

上面这个描述实际上不准确，论文中明确说明优化困难不是由于梯度消失而导致的。

### 什么是梯度消失？传统的激活函数在两端会进入梯度饱和区，从而导致梯度消失。但现代激活函数比如ReLU，他在输入为正时恒为1，那么应当就没有梯度消失问题才对啊？

ReLU 在输入为负数时恒为 0，因此对梯度消失的效果有限。ResNet 的梯度消失和爆炸的问题主要是通过 BN 和初始化来解决的。

<!-- ### BatchNorm 的公式及代码实现 -->
